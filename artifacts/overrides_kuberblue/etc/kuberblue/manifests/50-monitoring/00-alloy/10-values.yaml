# Grafana Alloy Configuration
# Unified telemetry collector for Kuberblue

# Alloy configuration
alloy:
  # Main configuration via ConfigMap
  configMap:
    create: true
    content: |
      // Logging configuration
      logging {
        level  = "info"
        format = "logfmt"
      }
      
      // ===== DISCOVERY =====
      // Kubernetes pod discovery
      discovery.kubernetes "pods" {
        role = "pod"
      }
      
      // Kubernetes service discovery
      discovery.kubernetes "services" {
        role = "service"
      }
      
      // Kubernetes node discovery
      discovery.kubernetes "nodes" {
        role = "node"
      }
      
      // ===== METRICS COLLECTION =====
      // Scrape Kubernetes pods with prometheus annotations
      prometheus.scrape "kubernetes_pods" {
        targets = discovery.relabel.kubernetes_pods.output
        forward_to = [prometheus.relabel.metrics.receiver]
        scrape_interval = "15s"
      }
      
      // Relabel for pod discovery
      discovery.relabel "kubernetes_pods" {
        targets = discovery.kubernetes.pods.targets
        
        // Only keep pods with prometheus scrape annotations
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_scrape"]
          regex = "true"
          action = "keep"
        }
        
        // Use pod annotation for metrics path
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_path"]
          target_label = "__metrics_path__"
          regex = "(.+)"
        }
        
        // Use pod annotation for scrape port
        rule {
          source_labels = ["__address__", "__meta_kubernetes_pod_annotation_prometheus_io_port"]
          regex = "([^:]+)(?::\\d+)?;(\\d+)"
          replacement = "$1:$2"
          target_label = "__address__"
        }
        
        // Add pod labels
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
        
        // Add pod metadata
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
      }
      
      // Scrape kubelet metrics
      prometheus.scrape "kubelet" {
        targets = discovery.relabel.kubelet.output
        forward_to = [prometheus.relabel.metrics.receiver]
        scheme = "https"
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
        }
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      }
      
      // Relabel for kubelet discovery
      discovery.relabel "kubelet" {
        targets = discovery.kubernetes.nodes.targets
        
        rule {
          target_label = "__address__"
          replacement = "kubernetes.default.svc.cluster.local:443"
        }
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          target_label = "__metrics_path__"
          replacement = "/api/v1/nodes/${1}/proxy/metrics"
        }
      }
      
      // Scrape cAdvisor metrics
      prometheus.scrape "cadvisor" {
        targets = discovery.relabel.cadvisor.output
        forward_to = [prometheus.relabel.metrics.receiver]
        scheme = "https"
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
        }
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      }
      
      // Relabel for cAdvisor discovery
      discovery.relabel "cadvisor" {
        targets = discovery.kubernetes.nodes.targets
        
        rule {
          target_label = "__address__"
          replacement = "kubernetes.default.svc.cluster.local:443"
        }
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          target_label = "__metrics_path__"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/cadvisor"
        }
      }
      
      // Metrics relabeling before sending to Mimir
      prometheus.relabel "metrics" {
        forward_to = [prometheus.remote_write.mimir.receiver]
        
        // Add cluster label
        rule {
          target_label = "cluster"
          replacement = "kuberblue"
        }
      }
      
      // Remote write to Mimir
      prometheus.remote_write "mimir" {
        endpoint {
          url = "http://mimir-distributor:8080/api/v1/push"
          
          queue_config {
            max_samples_per_send = 1000
            batch_send_deadline = "5s"
          }
        }
      }
      
      // ===== LOGS COLLECTION =====
      // Collect Kubernetes pod logs
      loki.source.kubernetes "pods" {
        targets    = discovery.kubernetes.pods.targets
        forward_to = [loki.process.pods.receiver]
      }
      
      // Process pod logs
      loki.process "pods" {
        forward_to = [loki.write.default.receiver]
        
        // Extract level from logs
        stage.regex {
          expression = "(?P<level>(DEBUG|INFO|WARN|ERROR|FATAL))"
        }
        
        // Add level label
        stage.labels {
          values = {
            level = "",
          }
        }
        
        // Add cluster label
        stage.static_labels {
          values = {
            cluster = "kuberblue",
          }
        }
      }
      
      // Collect system logs
      loki.source.journal "system" {
        forward_to = [loki.process.system.receiver]
        labels = {
          job = "systemd-journal",
        }
      }
      
      // Process system logs
      loki.process "system" {
        forward_to = [loki.write.default.receiver]
        
        // Add cluster label
        stage.static_labels {
          values = {
            cluster = "kuberblue",
          }
        }
      }
      
      // Write logs to Loki (minimal config)
      loki.write "default" {
        endpoint {
          url = "http://loki-gateway/loki/api/v1/push"
        }
      }
      
      // ===== TRACES COLLECTION =====
      // OTLP receiver for traces
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        
        http {
          endpoint = "0.0.0.0:4318"
        }
        
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs    = [otelcol.processor.batch.default.input]
          traces  = [otelcol.processor.batch.default.input]
        }
      }
      
      // Batch processor
      otelcol.processor.batch "default" {
        output {
          metrics = [otelcol.exporter.prometheus.default.input]
          logs    = [otelcol.exporter.loki.default.input]
          traces  = [otelcol.exporter.otlp.tempo.input]
        }
        
        send_batch_size = 1000
        timeout = "5s"
      }
      
      // Export metrics to Prometheus/Mimir
      otelcol.exporter.prometheus "default" {
        forward_to = [prometheus.remote_write.mimir.receiver]
      }
      
      // Export logs to Loki
      otelcol.exporter.loki "default" {
        forward_to = [loki.write.default.receiver]
      }
      
      // Export traces to Tempo
      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo:4317"
          tls {
            insecure = true
          }
        }
      }
      
      // ===== SELF MONITORING =====
      // Expose Alloy's own metrics
      prometheus.exporter.self "alloy" {
        
      }
      
      // Scrape Alloy's own metrics
      prometheus.scrape "alloy" {
        targets    = prometheus.exporter.self.alloy.targets
        forward_to = [prometheus.remote_write.mimir.receiver]
        job_name   = "alloy"
      }

# Controller configuration
controller:
  # Deploy as DaemonSet for node-level collection
  type: daemonset
  
  # Volume mounts - using extra section as expected by chart
  volumeMounts:
    extra:
      - name: varlog
        mountPath: /var/log
        readOnly: true
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true
  
  # Volumes - using extra section as expected by chart
  volumes:
    extra:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

# Resources
resources:
  requests:
    cpu: 25m
    memory: 64Mi
  limits:
    cpu: 100m
    memory: 128Mi

# Service for OTLP receiver
service:
  enabled: true
  type: ClusterIP
  clusterIP: ""

# RBAC
rbac:
  create: true

# Service account
serviceAccount:
  create: true

# Pod security
podSecurityContext:
  runAsUser: 0  # Required for journal access
  runAsGroup: 0
  fsGroup: 0

# Container security
containerSecurityContext:
  privileged: true  # Required for journal access
  capabilities:
    add:
      - SYS_TIME  # For journal access

# Monitoring
serviceMonitor:
  enabled: false  # Disabled - Alloy-first architecture uses native self-monitoring

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "12345"

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# Tolerations to run on all nodes
tolerations:
  - effect: NoSchedule
    operator: Exists

# Node selector
nodeSelector: {}

# Affinity
affinity: {}